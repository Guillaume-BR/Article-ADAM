

\documentclass[11pt,a4paper, french]{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue, 
    citecolor=blue
    }

\urlstyle{same}

\usepackage[T1]{fontenc} 
\usepackage[francais]{babel} 
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{pdfpages}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[margin=1.5cm, top=1cm, bottom=1.5cm]{geometry}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{calc}
\usepackage{diagbox}
\usepackage{bbold}
\usepackage{array}
\usepackage{minted}
\usepackage{alltt}
\usepackage{pstricks-add}
\usepackage{pstricks}
\usepackage{amsthm}
% \usepackage{moreverb}
\usepackage{forest}
\usepackage{tikz}
\usepackage{enumerate}


\usepackage[ruled,lined]{algorithm2e}

\usepackage{float}
\usepackage[caption = false]{subfig}


\usepackage{url}

\usetikzlibrary{automata}
\usetikzlibrary{calc,arrows.meta,positioning}

\title{\textsc{ADAM: A method for Stochastic Optimization}\\
Résumé d'article}
\author{Guillaume BERNARD-REYMOND, Guillaume BOULAND,\\ Camille MOTTIER, Abel SILLY}
\date{26 septembre 2024}
% \newcommand{\iddots}{\reflectbox{$\ddots$}}


\usetikzlibrary{arrows.meta}

\newcommand{\R}{\mathbb{R}}
\newcommand{\m}{\mathcal}
\newcommand{\un}{\mathbb{1}_n}
\newcommand{\N}{\mathbb{N}}

\newcommand{\vs}[1]{\vspace{#1cm}}
\newcommand{\hs}[1]{\hspace{#1cm}}
\newcommand{\dsum}[2]{\displaystyle\sum_{#1}^{#2}}
\newcommand{\dint}[2]{\displaystyle\int_{#1}^{#2}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}

\newcommand{\var}{\mathbb{V}\text{ar}}
\newcommand{\cov}{\mathbb{C}\text{ov}}

\setlength{\parindent}{0pt}
\frenchbsetup{StandardLists=true}

\theoremstyle{definition}

\newtheorem{Qu}{Question}


\begin{document}

\maketitle

\

\section{Introduction}

L'article étudié ici, intitulé \og ADAM: A method for Stochastic Optimization \fg{}, a été écrit en 2015 par Diederik P. Kingma (Université d'Amsterdam, OpenAI) et Jimmy Lei Ba (Université de Toronto), dans le cadre d'une conférence à l'International Conference on Learning Representations (ICLR). 

\

Cet article présente l'algorithme Adam, algorithme d'optimisation stochastique, basée sur une descente de gradient, dans le cadre d'un espace de paramètres à grande dimension.
Outre le fait que cet algorithme est simple à implémenter, efficace computationnellement et nécessite peu de mémoire, il offre une méthode qui marche bien dans un large panel de cas, y compris dans les cas problématiques de gradients éparses ou de fonctions-objectifs non stationnaires. En cela, il combine les qualités d'algorithmes existants au préalable, tels que AdaGrad et RMSProp.

\

Cet article présente une description précise de l'algorithme Adam, fournit un résultat de convergence de la méthode et détaille l'apport de l'algorithme Adam vis-à-vis d'autres algorithmes. 



\section{Algorithme}

On considère une fonction-objectif stochastique $f(\theta)$ de paramètres $\theta$, qu'on suppose différentiable. On souhaite optimiser les paramètres $\theta$ afin de minimiser l'espérance $\mathbb E[f(\theta)]$. \\
Outre la fonction $f(\theta)$, l'algorithme nécessite la donnée d'un pas $\alpha$, de taux $\beta_1,\beta_2\in[0,1[$, d'une constante de stabilisation numérique $\varepsilon>0$ et de paramètres initiaux $\theta_0$. Il exécute alors le schéma suivant :


\begin{algorithm}
  \caption{Adam}
  \Entree{$f(\theta)$, $\alpha$, $\beta_1$, $\beta_2$, $\varepsilon$, $\theta_0$}
  $m_0\longleftarrow 0$ \\
  $v_0\longleftarrow 0$ \\  
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $m_t \longleftarrow \beta_1\cdotp m_{t-1}+(1-\beta_1)\cdotp g_t$\\
    $v_t \longleftarrow \beta_2\cdotp v_{t-1}+(1-\beta_2)\cdotp g_t^2$\\
    $\widehat m_t \longleftarrow m_t/(1-\beta_1^t)$\\
    $\widehat v_t \longleftarrow v_t/(1-\beta_2^t)$\\
    $\theta_t \longleftarrow \theta_{t-1}-\alpha\cdotp \widehat m_t/(\sqrt{\widehat v_t}+\varepsilon)$
  }
  \Sortie{$\theta_t$}
\end{algorithm}

Pour comprendre cet algorithme et identifier les apports de la méthode Adam, nous pouvons le mettre en relation avec d'autres algorithmes de descente de gradient stochastique, présentés dans l'annexe. Nous pouvons maintenant interpréter les étapes de l'algorithme Adam et identifier les nouveautés. 

\bit
\item $m_t \longleftarrow \beta_1\cdotp m_{t-1}+(1-\beta_1)\cdotp g_t$\\
Fournit une estimation de $\mathbb E[g_t]$ par moyenne mobile à décroissance exponentielle : $m_t=(1-\beta_1)\dsum{i=1}t\beta_1^{t-i}\cdotp g_i$.\\
Permet de garder mémoire des gradients précédents pour atténuer les variations.\\
On trouve une idée similaire dans l'algorithme SGD avec moment.

\item $v_t \longleftarrow \beta_2\cdotp v_{t-1}+(1-\beta_2)\cdotp g_t^2$\\
Permettra, lors de la mise à jour des paramètres, une mise à l'échelle du gradient, c'est-à-dire qu'on ne va pas utiliser le même pas pour tous les paramètres. On ralentit le pas pour un paramètre qui a beaucoup changé précédemment, et on l'accélère pour un paramètre qui a peu changé. {\red est-ce bien ça ? c'est le paramètre qui change ou la fonction qu'il fait changer ?!}.\\
On utilise ici encore une estimation de $\mathbb E[g_t^2]$ obtenue par moyenne mobile à décroissance exponentielle.
On trouve une idée similaire dans les algorithmes AdaGrad (sans moyenne mobile) et RMSProp. 

\item  $\widehat m_t \longleftarrow m_t/(1-\beta_1^t)$ et 
$\widehat v_t \longleftarrow v_t/(1-\beta_2^t)$\\
Permet de corriger le biais vers 0 venant de l'initialisation de $m_0$ et $v_0$ à 0. \\
C'est une innovation fournie par cet algorithme. 
\eit

En conclusion, on peut voir que cet algorithme combine les idées fournies par les précédents algorithmes de descente de gradient, tels que la personnalisation des pas à chaque paramètre et la mémorisation du passé par le biais des moyennes mobiles, tout en apportant un vrai élément : la correction des biais des moments d'ordre 1 et 2. 



\section{Efficacité et points forts de la méthode}

\section{Amélioration des méthodes AdaGrad et RMSProp}

+ visu

+ biblio ?!!

\section{Annexes}




\begin{algorithm}
  \caption{SGD}
  \Entree{$f(\theta)$, $\alpha$, $\theta_0$}
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $\theta_t \longleftarrow \theta_{t-1}-\alpha \cdotp g_t(\theta_{t-1})$
  }
  \Sortie{$\theta_t$}
\end{algorithm}

\begin{algorithm}
  \caption{SGD avec moment (1964)}
  \Entree{$f(\theta)$, $\alpha$, $\rho$, $\theta_0$} 
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $m_t \longleftarrow \rho \cdotp m_{t-1}-\alpha \cdotp g_t$\\
    $\theta_t \longleftarrow \theta_{t-1}+m_t$
  }
  \Sortie{$\theta_t$}
\end{algorithm}

\begin{algorithm}
  \caption{AdaGrad (2011)}
  \Entree{$f(\theta)$, $\alpha$, $\varepsilon$, $\theta_0$}
  $v_0\longleftarrow 0$ \\  
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $v_t \longleftarrow  v_{t-1}+ g_t^2$\\
    $\theta_t \longleftarrow \theta_{t-1}-\alpha\cdotp g_t/(\sqrt{ v_t}+\varepsilon)$
  }
  \Sortie{$\theta_t$}
\end{algorithm}

\begin{algorithm}
  \caption{RMSProp (2012)}
  \Entree{$f(\theta)$, $\alpha$, $\beta_2$, $\varepsilon$, $\theta_0$}
  $v_0\longleftarrow 0$ \\  
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $v_t \longleftarrow \beta_2\cdotp v_{t-1}+(1-\beta_2)\cdotp g_t^2$\\
    $\theta_t \longleftarrow \theta_{t-1}-\alpha\cdotp g_t/(\sqrt{v_t}+\varepsilon)$
  }
  \Sortie{$\theta_t$}
\end{algorithm}



\end{document}

