\documentclass[11pt,aspectratio=169,xcolor=dvipsnames, french]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel} 
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}
\usetheme{General}
%\usetheme{Montpellier}



\usepackage[ruled,lined]{algorithm2e}



\title{ADAM : Une méthode pour l'optimisation stochastique}
\author{Guillaume BERNARD-REYMOND, Guillaume BOULAND,\\ Camille MOTTIER, Abel SILLY}
\date{14 octobre 2024}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

\begin{document}

\frame{\titlepage}

\begin{frame}{L'article}
\begin{center}
 \textbf{Adam : A Method for Stochastic Optimization}

\begin{multicols}{2}

 Diederik P. Kingma\\
 {\small Université d'Amsterdam, OpenAI}
 
 \columnbreak
 
 Jimmy Lei Ba\\
 {\small Université de Toronto}
\end{multicols}

Première année de publication : 2014
\end{center}

\end{frame}

\begin{frame}{L'objectif}
$f(\theta)$ : fonction-objectif stochastique 

\

Exemple : $f(\theta)=\displaystyle\sum_{i=1}^{n}L(x_i|\theta)$ \hspace{2cm}
Mini-batch : $f_t(\theta)=\displaystyle \sum_{i\in I_t}L(x_i|\theta)$

\

Objectif : minimiser $\mathbb E[f(\theta)]$
\end{frame}

\begin{frame}{Méthode d'ordre 1}
Rappel : Qu'est-ce qu'une méthode d'ordre 1 ? Évaluation de $f(\theta)$ et de $\nabla_{\theta}f(\theta)$

\begin{center}
\scalebox{0.8}{
\begin{algorithm}[H]
  \caption{Descente de gradient stochastique}
  \Entree{$f(\theta)$, $\alpha$, $\theta_0$}
  $t\longleftarrow 0$ \\
  \Tq{$\theta_t$ ne converge pas}{
    $t\longleftarrow t+1$\\
    $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
    $\theta_t \longleftarrow \theta_{t-1}-\alpha \cdotp g_t(\theta_{t-1})$
  }
  \Sortie{$\theta_t$}
\end{algorithm}}

\end{center}

\end{frame}




\begin{frame}{L'algorithme}

\textbf{Entrées :} $f(\theta)$, $\alpha$, $\beta_1$, $\beta_2$, $\varepsilon$, $\theta_0$\\
\hspace*{0.5cm}   $m_0\longleftarrow 0$ \\
\hspace*{0.5cm}  $v_0\longleftarrow 0$ \\  
\hspace*{0.5cm}  $t\longleftarrow 0$ \\
\hspace*{0.5cm}Tant que $\theta_t$ ne converge pas\\
   \hspace*{1.5cm} $t\longleftarrow t+1$\\
   \hspace*{1.5cm} $g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$\\
  \hspace*{1.5cm}  $m_t \longleftarrow \beta_1\cdotp m_{t-1}+(1-\beta_1)\cdotp g_t$\\
   \hspace*{1.5cm} $v_t \longleftarrow \beta_2\cdotp v_{t-1}+(1-\beta_2)\cdotp g_t^2$\\
   \hspace*{1.5cm} $\widehat m_t \longleftarrow m_t/(1-\beta_1^t)$\\
   \hspace*{1.5cm} $\widehat v_t \longleftarrow v_t/(1-\beta_2^t)$\\
   \hspace*{1.5cm} $\theta_t \longleftarrow \theta_{t-1}-\alpha\cdotp \widehat m_t/(\sqrt{\widehat v_t}+\varepsilon)$
  
\hspace*{0.5cm} \textbf{Sorties :} $\theta_t$
\end{frame}

\begin{frame}{Explications}

$f_1,\ldots, f_T$ : fonction $f$ restreinte à des mini-batches aléatoires \\

\pause

$g_t \longleftarrow \nabla_{\theta}f_t(\theta_{t-1})$ : mise à jour du gradient \\

 \pause
 
$\left.
\begin{array}{ll}
 m_t \longleftarrow \beta_1\cdotp m_{t-1}+(1-\beta_1)\cdotp g_t \\  
v_t \longleftarrow \beta_2\cdotp v_{t-1}+(1-\beta_2)\cdotp g_t^2   
\end{array}
\right\rbrace$ : mise à jour des moments d'ordre 1 et 2 par moyenne mobile \\


 
 

\pause

$\left.
\begin{array}{ll}
\widehat m_t \longleftarrow m_t/(1-\beta_1^t) \\ 
  \widehat v_t \longleftarrow v_t/(1-\beta_2^t)      
\end{array}
\right\rbrace$ : débiaisage des moments d'ordre 1 et 2 

 


 

 \pause

 $\theta_t \longleftarrow \theta_{t-1}-\alpha\cdotp \widehat m_t/(\sqrt{\widehat v_t}+\varepsilon)$ : Mise à jour de $\theta_t$

    
\end{frame}

\begin{frame}{Résultats de convergence}

On définit le regret par : $$R(T)=\sum_{t=1}^{T}f_t(x_t)-\min_{x\in\mathcal{X}}\sum_{t=1}^{T}f_t(x)$$

\begin{theorem}
  Sous certaines hypothèses de majoration du gradient de $f_t$, et de l'écart entre les valeurs de $\theta_n$ on a : 
    $$\frac{R(T)}{T}=O\left(\frac{1}{\sqrt{T}}\right)$$  
\end{theorem}

\end{frame}

\begin{frame}{Simulations numériques}
    Mettre images et commenter
\end{frame}

\end{document}
