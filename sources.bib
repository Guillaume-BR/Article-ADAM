@misc{starmer2023optimization,
  author    = {Josh Starmer},
  title     = {Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)},
  year      = {2023},
  howpublished       = {\url{https://www.youtube.com/watch?v=NE88eqLngkg}},
  howpublished = {YouTube},
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
    howpublished = {\url{https://arxiv.org/abs/1412.6980}}
}

@misc{dereich2024convergenceratesadamoptimizer,
      title={Convergence rates for the Adam optimizer}, 
      author={Steffen Dereich and Arnulf Jentzen},
      year={2024},
      eprint={2407.21078},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2407.21078}, 
      howpublished = {\url{https://arxiv.org/abs/2407.21078}}
}

@misc{github,
  title = {Github du projet},
  howpublished = {\url{https://github.com/Guillaume-BR/Article-ADAM}},
author = {G. Bernard-Reymond and G. Bouland and C. Mottier and A.Silly},
year={2024}
}
